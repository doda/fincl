{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp run_bt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\doda\\Anaconda3\\envs\\fincl2\\lib\\site-packages\\sklearn\\utils\\deprecation.py:144: FutureWarning: The sklearn.ensemble.bagging module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.ensemble. Anything that cannot be imported from sklearn.ensemble is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "C:\\Users\\doda\\Anaconda3\\envs\\fincl2\\lib\\site-packages\\sklearn\\utils\\deprecation.py:144: FutureWarning: The sklearn.ensemble.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.ensemble. Anything that cannot be imported from sklearn.ensemble is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# export\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "import random\n",
    "from datetime import date\n",
    "from dateutil.relativedelta import relativedelta\n",
    "    \n",
    "from mlbt.load_data import (\n",
    "    get_symbols,\n",
    "    load_and_sample_bars,\n",
    "    load_bars,\n",
    "    save_bars,\n",
    "    load_events_b,\n",
    "    save_events_b,\n",
    "    load_feat,\n",
    "    save_feat,\n",
    "    load_imp,\n",
    "    save_imp,\n",
    "    load_payload,\n",
    "    save_payload,\n",
    ")\n",
    "from mlbt.filters import cusum\n",
    "from mlbt.multiprocess import mp_pandas_obj\n",
    "from mlbt.utils import get_daily_vol, NumpyEncoder\n",
    "from mlbt.get_bins import get_bins, drop_labels\n",
    "from mlbt.alpha import ma_alpha, bb_alpha\n",
    "from mlbt.binarize import triple_barrier_method, fixed_horizon\n",
    "from mlbt.feature_eng import run_feature_engineering, define_feature_configs\n",
    "from mlbt.reporting import get_reports\n",
    "from mlbt.models import get_model\n",
    "from mlbt.feature_importance import feat_importance\n",
    "\n",
    "\n",
    "try:\n",
    "    from mlbt import settings_pers as settings\n",
    "except:\n",
    "    from mlbt import settings\n",
    "\n",
    "FORMAT = \"%(asctime)-15s %(message)s\"\n",
    "logging.basicConfig(format=FORMAT, level=logging.DEBUG)\n",
    "\n",
    "SYMBOL_GROUPS = {\n",
    "    \"agriculture\": \"Agriculture\",\n",
    "    \"currency\": \"Currency\",\n",
    "    \"energy\": \"Energy\",\n",
    "    \"equity_index\": \"Equity Index\",\n",
    "    \"interest_rate\": \"Interest Rate\",\n",
    "    \"metals\": \"Metals\",\n",
    "}\n",
    "\n",
    "\n",
    "def downsample(bars, type_, daily_vol):\n",
    "    if type_ == \"cusum\":\n",
    "        return cusum(bars[\"Close\"], daily_vol.mean())\n",
    "\n",
    "    return bars.index\n",
    "\n",
    "\n",
    "def alpha(bars, events, type_, params):\n",
    "    if type_ == \"none\":\n",
    "        return events\n",
    "    elif type_ == \"ma-cross\":\n",
    "        signal = ma_alpha(bars, *params)\n",
    "    elif type_ == \"bbands-mr\":\n",
    "        signal = bb_alpha(bars, *params, True)\n",
    "    elif type_ == \"bbands-tf\":\n",
    "        signal = bb_alpha(bars, *params, False)\n",
    "\n",
    "    events[\"side\"] = signal\n",
    "\n",
    "    assert set(events[\"side\"].dropna()) == set([1, -1]), set(events[\"side\"].dropna())\n",
    "    return events\n",
    "\n",
    "\n",
    "def pick_good_features(imp_all, columns):\n",
    "    \"\"\"Pick features that help our classifier's predictive abilities\"\"\"\n",
    "    imp_d = imp_all[\"mean\"].to_dict()\n",
    "    cutoff = imp_all[\"mean\"].mean()\n",
    "    picked_cols = [col for col in columns if imp_d[col] > cutoff]\n",
    "\n",
    "    logging.info(f\"Picked {len(picked_cols)}/{len(columns)} important features: {picked_cols}\")\n",
    "\n",
    "    return picked_cols\n",
    "\n",
    "\n",
    "def combine_symbol_decks(deck):\n",
    "    \"\"\"\n",
    "    Join events, features and bins that have been computed on a per-symbol level into one\n",
    "    grand data-frame. To note: In order to in the future still be able to differentiate which row belongs\n",
    "    to which symbol we embed the symbols position in our symbols table into the microseconds of the index.\n",
    "    This is not in any way good code, but it allows us to still have a unique & sortable index without\n",
    "    resorting to multi-indices or the like. This is predicated on the fact that we know we only sample from\n",
    "    1-minute bars. A.k.a Poor Man's Multi-Index\n",
    "    \"\"\"\n",
    "    all_events, all_feats, all_bins = [], [], []\n",
    "    for i, (symbol, symbol_deck) in enumerate(deck.items()):\n",
    "        events, feats, bins = symbol_deck['events'], symbol_deck['feats'], symbol_deck['bins']\n",
    "\n",
    "        # every row for every symbol has a unique datetime index and is sortable\n",
    "        for df in [events, feats, bins]:\n",
    "            df.index += pd.Timedelta(i, \"us\")\n",
    "\n",
    "        events[\"t1\"] += pd.Timedelta(i, \"us\")\n",
    "        \n",
    "        all_events.append(events)\n",
    "        all_feats.append(feats)\n",
    "        all_bins.append(bins)\n",
    "\n",
    "    all_events = pd.concat(all_events).sort_index()\n",
    "    all_feats = pd.concat(all_feats).sort_index()\n",
    "    all_bins = pd.concat(all_bins).sort_index()\n",
    "\n",
    "    return all_events, all_feats, all_bins\n",
    "\n",
    "\n",
    "def train_test_split(events, feats, bins, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Exclude rows from our engineered features which haven't completed the warmup for all feature columns\n",
    "    and split the set 50/50 into train & test set\n",
    "    \"\"\"\n",
    "    X_all = feats\n",
    "    y_all = bins\n",
    "\n",
    "    # Drop all rows where we don't have a complete set of features\n",
    "    merged = pd.merge(X_all, y_all, left_index=True, right_index=True)\n",
    "    merged[(merged == np.inf) | (merged == -np.inf)] = np.nan\n",
    "    merged = merged.dropna()\n",
    "    merged = merged.truncate(before=start_date, after=end_date)\n",
    "    assert merged.shape[0] > 0\n",
    "\n",
    "    X_all = merged.drop(columns=bins.columns)\n",
    "    y_all = merged[\"bin\"]\n",
    "\n",
    "    events_all = events[events.index.isin(X_all.index)]\n",
    "    # Store all-kinds-of-information in events for later PnL calculations\n",
    "    events_all[bins.columns] = merged[bins.columns]\n",
    "\n",
    "    cut = X_all.shape[0] // 2\n",
    "    logging.info(f\"cut {merged.index[0].date()} << {merged.index[cut].date()} >> {merged.index[-1].date()}\")\n",
    "\n",
    "    events_train, events_test = events_all.iloc[:cut], events_all.iloc[cut:]\n",
    "    X_train, X_test = X_all.iloc[:cut], X_all.iloc[cut:]\n",
    "    y_train, y_test = y_all.iloc[:cut], y_all.iloc[cut:]\n",
    "    assert X_train.shape[0] > 0\n",
    "\n",
    "    logging.info(f\"events {events.shape}, feats {feats.shape}, bins {bins.shape}, X_all {X_all.shape}, X_train {X_train.shape}\")\n",
    "\n",
    "    return (events_train, X_train, y_train, events_test, X_test, y_test)\n",
    "\n",
    "\n",
    "def binarize(bars, t_events, type_, binarize_params, daily_vol, num_threads):\n",
    "    \"\"\"\n",
    "    Binarize the rows, i.e. for every row determine a forward returns window which\n",
    "    is then used to calculate that row's label\n",
    "    \"\"\"\n",
    "    if type_ == \"fixed_horizon\":\n",
    "        return fixed_horizon(t_events, binarize_params, daily_vol)\n",
    "    elif type_ == \"triple_barrier_method\":\n",
    "        return triple_barrier_method(\n",
    "            bars, t_events, binarize_params, daily_vol, num_threads\n",
    "        )\n",
    "\n",
    "def standard_scale(X_train, X_test):\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    X_train = pd.DataFrame(scaler.fit_transform(X_train), index=X_train.index)\n",
    "    X_test = pd.DataFrame(scaler.transform(X_test), index=X_test.index)\n",
    "    return X_train, X_test\n",
    "\n",
    "def pca_transform(X_train, X_test):\n",
    "    from sklearn.decomposition import PCA\n",
    "    pca = PCA(X_train.columns.shape[0] // 2)\n",
    "    X_train = pd.DataFrame(pca.fit_transform(X_train), index=X_train.index)\n",
    "    X_test = pd.DataFrame(pca.transform(X_test), index=X_test.index)\n",
    "    return X_train, X_test\n",
    "\n",
    "\n",
    "def prepare_payload(config, symbols, imp_all, reports):\n",
    "    \"\"\"Prepare payload for serialization\"\"\"\n",
    "    config[\"start_date\"] = config[\"start_date\"].isoformat()\n",
    "    config[\"end_date\"] = config[\"end_date\"].isoformat()\n",
    "    del config['symbols_map']\n",
    "\n",
    "    return {\n",
    "        \"symbols\": symbols,\n",
    "        \"feature_importance\": imp_all.to_dict(),\n",
    "        \"config\": config,\n",
    "        **reports,\n",
    "    }\n",
    "\n",
    "\n",
    "def get_symbols_list(config):\n",
    "    if config[\"symbols\"]:\n",
    "        symbols = config[\"symbols\"]\n",
    "    else:\n",
    "        symbols = get_symbols(config)\n",
    "\n",
    "    symbols = [x for x in symbols if x not in IGNORE_SYMBOLS]\n",
    "    return symbols\n",
    "    \n",
    "\n",
    "def abort_early(config):\n",
    "    symbols = get_symbols_list(config)\n",
    "    if not symbols:\n",
    "        logging.info(\"No valid symbols, aborting...\")\n",
    "        return True\n",
    "    if config[\"check_completed\"]:\n",
    "        payload = load_payload(symbols, config)\n",
    "        if payload is not None:\n",
    "            logging.info(\"We have the payload, not recomputing\")\n",
    "            return True\n",
    "    return False\n",
    "    \n",
    "\n",
    "def parse_config(data):\n",
    "    \"\"\"Turn the input parameters into the config object which is used as configuration throughout the project\"\"\"\n",
    "    alpha, *alpha_params = data[\"alpha\"].split(\"_\")\n",
    "\n",
    "    alpha_params = [float(x) if \".\" in x else int(x) for x in alpha_params]\n",
    "\n",
    "    default_binarize_params = {\"triple_barrier_method\": [1, 1, 1], \"fixed_horizon\": 100}\n",
    "    binarize_params = data.get(\"binarize_params\") or default_binarize_params[data[\"binarize\"]]\n",
    "    \n",
    "    DATA_DIR = data.get(\"DATA_DIR\", settings.DATA_DIR)\n",
    "\n",
    "    return {\n",
    "        \"DATA_DIR\": DATA_DIR,\n",
    "        \"F_PAYLOAD_DIR\": data.get(\"F_PAYLOAD_DIR\", settings.F_PAYLOAD_DIR),\n",
    "        \"symbols_map\": pd.read_csv(DATA_DIR / \"symbols.csv\", index_col=\"iqsymbol\"),\n",
    "        \n",
    "        \"start_date\": data.get(\"start_date\", date(2000, 1, 1)),\n",
    "        \"end_date\": data.get(\"end_date\", date(2020, 1, 1)),\n",
    "        \"data_freq\": data.get(\"data_freq\", \"minutely\"),\n",
    "        \"downsampling\": data.get(\"downsampling\", \"cusum\"),\n",
    "        \"vol_estimate\": 100,\n",
    "        \"symbols\": data.get(\"symbols\"),\n",
    "        \"symbol_groups\": data.get(\"symbol_groups\"),\n",
    "        \"features\": data.get(\"features\", define_feature_configs()),        \n",
    "        \"test_procedure\": \"walk_forward\",\n",
    "        \"classifier\": data[\"classifier\"],\n",
    "        \"bar_type\": data[\"bar_type\"],\n",
    "        \"bar_size\": None,\n",
    "        \"binarize\": data[\"binarize\"],\n",
    "        \"binarize_params\": binarize_params,\n",
    "        \"alpha\": alpha,\n",
    "        \"alpha_params\": alpha_params,\n",
    "        \"feature_calc_only\": data.get(\"feature_calc_only\", False),\n",
    "        \"feature_imp_only\": data.get(\"feature_imp_only\", False),\n",
    "        \"skip_feature_imp\": data.get(\"skip_feature_imp\", False),\n",
    "        \"reuse_hypers\": data.get(\"reuse_hypers\", True),\n",
    "        \"hypers_n_iter\": data.get(\"hypers_n_iter\", 25),\n",
    "        \"load_from_disk\": data.get(\"load_from_disk\", True),\n",
    "        \"save_to_disk\": data.get(\"save_to_disk\", True),\n",
    "        \"optimize_hypers\": data.get(\"optimize_hypers\", True),\n",
    "        \"feat_imp_method\": data.get(\"feat_imp_method\", \"MDA\"),\n",
    "        \"feat_imp_n_estimators\": data.get(\"feat_imp_n_estimators\", 1000),\n",
    "        \"feat_imp_cv\": data.get(\"feat_imp_cv\", 10),\n",
    "        \"num_threads\": data.get(\"num_threads\", 32),\n",
    "        \"n_jobs\": data.get(\"n_jobs\", 4),\n",
    "        \"check_completed\": data.get(\"check_completed\", False),\n",
    "        \"pca_transform\": data.get(\"pca_transform\", False),\n",
    "        \"standard_scale\": data.get(\"standard_scale\", False),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "# TODO: Figure out why Lean Hogs break our code\n",
    "# TODO: Figure out why SP oversamples so mad in the far past\n",
    "IGNORE_SYMBOLS = [\"@LH#C\", \"@SP#C\"]\n",
    "\n",
    "\n",
    "def load_sample_and_binarize(config):\n",
    "    \"\"\"\n",
    "    Load our bars, chunk them into dollar bars aiming to have 50 bars per day per symbol for the year 2019.\n",
    "    These bars are then CUSUM downsampled and binarized before being saved for later runs.\n",
    "    \"\"\"\n",
    "    symbols = get_symbols_list(config)\n",
    "\n",
    "    logging.info(f\"Symbols: {symbols}\")\n",
    "    deck = {}\n",
    "    for symbol in symbols:\n",
    "        bars = load_bars(symbol, config)\n",
    "        if bars is None:\n",
    "            bars, bar_size = load_and_sample_bars(config, symbol, config[\"start_date\"], config[\"end_date\"], config[\"bar_type\"])\n",
    "            save_bars(symbol, config, bars)\n",
    "\n",
    "        events_b = load_events_b(symbol, config)\n",
    "        if events_b is None:\n",
    "            daily_vol = get_daily_vol(bars[\"Close\"], config[\"vol_estimate\"])\n",
    "            t_events = downsample(bars, config[\"downsampling\"], daily_vol)\n",
    "            logging.info(f\"{symbol}: Downsampled from {len(bars)} to {len(t_events)}\")\n",
    "\n",
    "            logging.debug(f\"{symbol}: Binarize {config['binarize']}\")\n",
    "            events_b = binarize(\n",
    "                bars,\n",
    "                t_events,\n",
    "                config[\"binarize\"],\n",
    "                config[\"binarize_params\"],\n",
    "                daily_vol,\n",
    "                config[\"num_threads\"],\n",
    "            )\n",
    "\n",
    "            save_events_b(symbol, config, events_b)\n",
    "\n",
    "        logging.info(f\"{symbol}: Have {bars.shape[0]} bars and {events_b.shape[0]} binarized events\")\n",
    "        deck[symbol] = {'bars': bars, 'events_b': events_b}\n",
    "\n",
    "    return deck\n",
    "\n",
    "def join_importances(dfs):\n",
    "    \"\"\"Join the feature importances computed parallelized & per-symbol into one dataframe\"\"\"\n",
    "    mean = pd.concat([x[\"mean\"] for x in dfs], axis=1).mean(axis=1)\n",
    "    std = pd.concat([x[\"std\"] for x in dfs], axis=1).std(axis=1) * len(dfs) ** -0.5\n",
    "    oos = pd.concat([x[\"oos\"] if \"oos\" in x else pd.Series(0, index=x.index) for x in dfs], axis=1).mean(axis=1)\n",
    "\n",
    "    return pd.DataFrame({\"mean\": mean, \"std\": std, \"oos\": oos})\n",
    "\n",
    "\n",
    "def feat_importance_mp_aux(indices, config, events_train, X_train, y_train):\n",
    "    imp = load_imp(indices, config)\n",
    "    if imp is None:\n",
    "        imp = feat_importance(\n",
    "            events_train.loc[indices],\n",
    "            X_train.loc[indices],\n",
    "            y_train.loc[indices],\n",
    "            n_estimators=config[\"feat_imp_n_estimators\"],\n",
    "            cv=config[\"feat_imp_cv\"],\n",
    "            method=config[\"feat_imp_method\"],\n",
    "            )\n",
    "        save_imp(indices, config, imp)\n",
    "    return imp\n",
    "\n",
    "\n",
    "def run_feat_importance(config, events_train, X_train, y_train):\n",
    "    assert events_train.index.equals(X_train.index)\n",
    "    assert events_train.index.equals(y_train.index)\n",
    "\n",
    "    imps = mp_pandas_obj(\n",
    "        feat_importance_mp_aux,\n",
    "        ('indices', events_train.index),\n",
    "        config=config,\n",
    "        events_train=events_train,\n",
    "        X_train=X_train,\n",
    "        y_train=y_train,\n",
    "        num_threads=config[\"num_threads\"],\n",
    "#         num_threads=1,\n",
    "#         mp_batches=config[\"num_threads\"],\n",
    "        combine_results=False,\n",
    "    )\n",
    "    return join_importances(imps)\n",
    "\n",
    "\n",
    "def prepare_alpha_bins_feature_imps(config, deck):\n",
    "    for symbol, symbol_deck in deck.items():\n",
    "        logging.debug(f\"{symbol}: Get bins and feature imps\")\n",
    "        \n",
    "        bars, events_b, feats = symbol_deck['bars'], symbol_deck['events_b'], symbol_deck['feats']\n",
    "        events = alpha(\n",
    "            bars, events_b, config[\"alpha\"], config[\"alpha_params\"]\n",
    "        )\n",
    "        # Store close_p here so we can do things with it in PnL analysis\n",
    "        events[\"close_p\"] = bars[\"Close\"][bars.index.isin(events.index)]\n",
    "\n",
    "        bins = get_bins(events, bars[\"Close\"])\n",
    "        bins = drop_labels(bins)\n",
    "\n",
    "        deck[symbol] = {'events': events, 'feats': feats, 'bins': bins}\n",
    "\n",
    "    return deck\n",
    "\n",
    "\n",
    "def run_ml_pipe(config, deck):\n",
    "    \"\"\"\n",
    "    Run the large chunk of our ML pipeline, which includes calculating the primary (and secondary) models,\n",
    "    splitting our data into train/test sets, calulating feature importances, hyper-parameter optimization,\n",
    "    model fitting and evaluation and generation of final reports which are later user for PnL simulations.\n",
    "    \"\"\"\n",
    "    if config[\"feature_imp_only\"]:\n",
    "        return\n",
    "\n",
    "    symbols = list(deck.keys())\n",
    "    for symbol, symbol_deck in deck.items():\n",
    "        logging.debug(f\"{symbol} {[symbol_deck[x].shape for x in ['events', 'feats', 'bins']]}\")\n",
    "    \n",
    "    events, feats, bins = combine_symbol_decks(deck)\n",
    "    e_x_y = train_test_split(events, feats, bins, config[\"start_date\"], config[\"end_date\"])\n",
    "    events_train, X_train, y_train, events_test, X_test, y_test = e_x_y\n",
    "\n",
    "    y_train, y_test = y_train.squeeze(), y_test.squeeze()\n",
    "    \n",
    "    if config[\"pca_transform\"]:\n",
    "        X_train, X_test = pca_transform(X_train, X_test)\n",
    "\n",
    "    if config[\"standard_scale\"]:\n",
    "        X_train, X_test = standard_scale(X_train, X_test)\n",
    "\n",
    "    if config['skip_feature_imp']:\n",
    "        imp_all = pd.DataFrame()\n",
    "    else:\n",
    "        imp_all = run_feat_importance(\n",
    "            config,\n",
    "            events_train,\n",
    "            X_train,\n",
    "            y_train,\n",
    "        )\n",
    "\n",
    "        # Important feats\n",
    "        cols = pick_good_features(imp_all, X_train.columns)\n",
    "        X_train, X_test = X_train[cols], X_test[cols]\n",
    "\n",
    "\n",
    "    del deck\n",
    "\n",
    "    hyper_params = None\n",
    "    # Try loading the payload so we can re-use hyper parameters from previous run\n",
    "    payload = load_payload(symbols, config)\n",
    "    if payload is not None:\n",
    "        if config[\"reuse_hypers\"]:\n",
    "            report = payload[\"secondary\"] or payload[\"primary\"]\n",
    "            hyper_params = report[\"hyper_params\"]\n",
    "            logging.info(f\"Loaded hypers\")\n",
    "\n",
    "    model, hyper_params = get_model(\n",
    "        events_train,\n",
    "        X_train,\n",
    "        y_train,\n",
    "        config[\"classifier\"],\n",
    "        config[\"optimize_hypers\"],\n",
    "        config[\"hypers_n_iter\"],\n",
    "        config[\"num_threads\"],\n",
    "        config[\"n_jobs\"],\n",
    "        hyper_params,\n",
    "    )\n",
    "    logging.info(f\"Using hypers {hyper_params}\")\n",
    "\n",
    "    reports = get_reports(\n",
    "        model,\n",
    "        events_test,\n",
    "        X_train,\n",
    "        y_train,\n",
    "        X_test,\n",
    "        y_test,\n",
    "        config[\"test_procedure\"],\n",
    "        config[\"alpha\"] != \"none\",\n",
    "        hyper_params,\n",
    "    )\n",
    "    \n",
    "    payload = prepare_payload(config, symbols, imp_all, reports)\n",
    "    save_payload(symbols, config, payload)\n",
    "\n",
    "    logging.info(f\"Run finished: Feature Importance OOS Score: {imp_all['oos'].mean() if 'oos' in imp_all else None}\")\n",
    "    logging.info(f\"F1 Score: {payload['primary']['f1_score']}\")\n",
    "\n",
    "    return payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "def run_bt(**data):\n",
    "    config = parse_config(data)\n",
    "    print_config = {k: v for k, v in config.items() if k not in 'symbols_map'}\n",
    "    logging.info(f\"config: {print_config}\")\n",
    "    \n",
    "    if abort_early(config):\n",
    "        return ''\n",
    "\n",
    "    # We store every symbol's data and computations in a central \"deck\" dictionary\n",
    "    deck = load_sample_and_binarize(config)\n",
    "    \n",
    "    deck = run_feature_engineering(config, deck)\n",
    "    if config['feature_calc_only']:\n",
    "        return ''\n",
    "\n",
    "    deck = prepare_alpha_bins_feature_imps(config, deck)\n",
    "    payload = run_ml_pipe(config, deck)\n",
    "    return payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = {\n",
    "    'start_date': date(2017, 1, 1),\n",
    "    'end_date': date(2021, 1, 1),\n",
    "    # \"symbol_groups\": [\"equity_index\"],\n",
    "#     \"symbol_groups\": list(SYMBOL_GROUPS.keys()),\n",
    "#     \"symbols\": [\"@ES#C\", \"@NQ#C\"],\n",
    "    \"symbols\": [\"@AD#C\"],\n",
    "    \"bar_type\": \"dollar\",\n",
    "    \"binarize\": \"fixed_horizon\",\n",
    "    \"binarize_params\": 50,\n",
    "    # \"alpha\": \"bbands-mr_500_1.5\",\n",
    "    # \"alpha\": \"ma-cross_51_500\",\n",
    "    \"alpha\": \"none\",\n",
    "#     \"classifier\": \"random_forest\",\n",
    "    # \"classifier\": \"xgboost\",\n",
    "    \"classifier\": \"lgbm\",\n",
    "    # \"classifier\": \"dummy\",\n",
    "    # \"classifier\": \"knn\",\n",
    "    # 'hypers_n_iter': 5,\n",
    "    # 'optimize_hypers': False,\n",
    "    # 'feature_imp_only': True,\n",
    "    # 'reuse_hypers': False,\n",
    "    'num_threads': 1,\n",
    "#     \"load_from_disk\": False,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-02-07 13:57:04,344 config: {'start_date': datetime.date(2017, 1, 1), 'end_date': datetime.date(2021, 1, 1), 'vol_estimate': 100, 'downsampling': 'cusum', 'symbols': ['@AD#C'], 'symbol_groups': None, 'test_procedure': 'walk_forward', 'classifier': 'lgbm', 'bar_type': 'dollar', 'bar_size': None, 'binarize': 'fixed_horizon', 'binarize_params': 50, 'alpha': 'none', 'alpha_params': [], 'feature_calc_only': False, 'feature_imp_only': False, 'skip_feature_imp': False, 'reuse_hypers': True, 'hypers_n_iter': 25, 'load_from_disk': True, 'save_to_disk': True, 'optimize_hypers': True, 'feat_imp_method': 'MDA', 'feat_imp_cv': 5, 'num_threads': 1, 'n_jobs': 4, 'check_completed': False, 'features': [{'name': 'log'}, {'name': 'close', 'symbol': 'VIX.XO'}, {'name': 'ffd', 'd': 0.5}, {'name': 'auto', 'window': 50, 'lag': 25}, {'name': 'stdev', 'window': 50}, {'name': 'roll', 'window': 50}, {'name': 'rollimp', 'window': 50}, {'name': 'amihud', 'window': 50}, {'name': 'kyle', 'window': 50}, {'name': 'volratio', 'com': 50}, {'name': 'auto', 'window': 250, 'lag': 25}, {'name': 'auto', 'window': 250, 'lag': 50}, {'name': 'stdev', 'window': 250}, {'name': 'roll', 'window': 250}, {'name': 'rollimp', 'window': 250}, {'name': 'amihud', 'window': 250}, {'name': 'kyle', 'window': 250}, {'name': 'volratio', 'com': 250}, {'name': 'auto', 'window': 500, 'lag': 25}, {'name': 'auto', 'window': 500, 'lag': 50}, {'name': 'auto', 'window': 500, 'lag': 250}, {'name': 'stdev', 'window': 500}, {'name': 'roll', 'window': 500}, {'name': 'rollimp', 'window': 500}, {'name': 'amihud', 'window': 500}, {'name': 'kyle', 'window': 500}, {'name': 'volratio', 'com': 500}, {'name': 'auto', 'window': 1000, 'lag': 25}, {'name': 'auto', 'window': 1000, 'lag': 50}, {'name': 'auto', 'window': 1000, 'lag': 250}, {'name': 'auto', 'window': 1000, 'lag': 500}, {'name': 'stdev', 'window': 1000}, {'name': 'roll', 'window': 1000}, {'name': 'rollimp', 'window': 1000}, {'name': 'amihud', 'window': 1000}, {'name': 'kyle', 'window': 1000}, {'name': 'volratio', 'com': 1000}]}\n",
      "2020-02-07 13:57:04,344 Symbols: ['@AD#C']\n",
      "2020-02-07 13:57:04,363 Note: NumExpr detected 32 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "2020-02-07 13:57:04,364 NumExpr defaulting to 8 threads.\n",
      "2020-02-07 13:57:04,427 @AD#C: Have 20488 bars and 401 binarized events\n",
      "2020-02-07 13:57:04,428 @AD#C: Feature engineering\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-61d9342b8c2e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mrun_bt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-5-a51adf66b0a7>\u001b[0m in \u001b[0;36mrun_bt\u001b[1;34m(**data)\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mdeck\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_sample_and_binarize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mdeck\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrun_feature_engineering\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdeck\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'feature_calc_only'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-ae90c051dcb5>\u001b[0m in \u001b[0;36mrun_feature_engineering\u001b[1;34m(config, deck)\u001b[0m\n\u001b[0;32m     61\u001b[0m         \u001b[0mdeck\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msymbol\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'feats'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeats3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m         \u001b[1;36m1\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdeck\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "run_bt(**conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[1;32m<ipython-input-4-ae90c051dcb5>\u001b[0m(63)\u001b[0;36mrun_feature_engineering\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m     61 \u001b[1;33m        \u001b[0mdeck\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msymbol\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'feats'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeats3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m     62 \u001b[1;33m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m---> 63 \u001b[1;33m        \u001b[1;36m1\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m     64 \u001b[1;33m    \u001b[1;32mreturn\u001b[0m \u001b[0mdeck\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m     65 \u001b[1;33m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> feat.shape\n",
      "(20488,)\n",
      "ipdb> feats2.shape\n",
      "(600349, 37)\n",
      "ipdb> feats3.shape\n",
      "(20488, 37)\n",
      "ipdb> exit\n"
     ]
    }
   ],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
