{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\doda\\anaconda3\\envs\\fincl\\lib\\site-packages\\sklearn\\utils\\deprecation.py:144: FutureWarning: The sklearn.ensemble.bagging module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.ensemble. Anything that cannot be imported from sklearn.ensemble is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "c:\\users\\doda\\anaconda3\\envs\\fincl\\lib\\site-packages\\sklearn\\utils\\deprecation.py:144: FutureWarning: The sklearn.ensemble.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.ensemble. Anything that cannot be imported from sklearn.ensemble is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# export\n",
    "\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import json\n",
    "import logging\n",
    "from path import Path\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from mlfinlab.data_structures import get_dollar_bars, get_tick_bars, get_time_bars, get_volume_bars\n",
    "\n",
    "from mlbt.utils import NumpyEncoder\n",
    "\n",
    "# You'll likely have to change these if you're intending to run the code yourself\n",
    "# TODO: Factor out into settings.py file\n",
    "# DATA_DIR = Path(\"~/Dropbox/algotrading/data\").expanduser()\n",
    "# F_PAYLOAD_DIR = Path(\"~/pr/fincl/frontend/public/payloads\").expanduser()\n",
    "\n",
    "# SYMBOLS_CSV = pd.read_csv(DATA_DIR / \"symbols.csv\", index_col=\"iqsymbol\")\n",
    "\n",
    "LENGTH_RANKING = ['@SP#C','@W#C','@BO#C','@QM#C','@QG#C','EZ#C','@KC#C','@C#C','XG#C','@S#C','EX#C','@SB#C','@OJ#C','@NKD#C','@EMD#C','BD#C','LF#C','QNG#C','QCL#C','@HE#C','@CC#C','@CT#C','@LE#C','@ES#C','@TY#C','QSI#C','QPL#C','@FV#C','@O#C','LG#C','@SM#C','@GF#C','QGC#C','@YM#C','@TU#C','QPA#C','@UB#C','@NQ#C','@ED#C','GAS#C','@MME#C','QHG#C','QHO#C','@BP#C','@CD#C','@RP#C','@NE#C','@AD#C']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "def get_symbols(config):\n",
    "    symbol_groups = config[\"symbol_groups\"]\n",
    "    lists = {\"us_index\": [\"@NQ#C\", \"@ES#C\", \"@YM#C\"]}\n",
    "    if len(symbol_groups) == 1 and symbol_groups[0] in lists:\n",
    "        return lists[symbol_groups[0]]\n",
    "\n",
    "    sectors = {\n",
    "        \"agriculture\": \"Agriculture\",\n",
    "        \"currency\": \"Currency\",\n",
    "        \"energy\": \"Energy\",\n",
    "        \"equity_index\": \"Equity Index\",\n",
    "        \"interest_rate\": \"Interest Rate\",\n",
    "        \"metals\": \"Metals\",\n",
    "    }\n",
    "    \n",
    "    symbol_groups = [sectors[x] for x in symbol_groups]\n",
    "    symbols_map = config['symbols_map']\n",
    "    picked = symbols_map[symbols_map[\"Sector\"].isin(symbol_groups)]\n",
    "\n",
    "    return picked.index.values\n",
    "\n",
    "\n",
    "def load_contract(config, contract_name, directory):\n",
    "    series = pd.read_csv(\n",
    "        config[\"DATA_DIR\"] / directory / \"{}.csv\".format(contract_name), index_col=0\n",
    "    )\n",
    "    series = series[::-1]\n",
    "    if directory == \"minutely\":\n",
    "        series[\"Time\"] = series[\"date\"] + \" \" + series[\"time\"]\n",
    "        series = series.set_index(\n",
    "            pd.to_datetime(series[\"Time\"], format=\"%Y-%m-%d 0 days %H:%M:00.000000000\")\n",
    "        )\n",
    "    else:\n",
    "        series[\"Time\"] = series[\"date\"]\n",
    "        series = series.set_index(pd.to_datetime(series[\"Time\"], format=\"%Y-%m-%d\"))\n",
    "\n",
    "    series = series[[\"open_p\", \"close_p\", \"prd_vlm\", \"Time\"]]\n",
    "    series = series.rename(\n",
    "        columns={\"close_p\": \"Close\", \"open_p\": \"Open\", \"prd_vlm\": \"Volume\"}\n",
    "    )\n",
    "    series[\"Instrument\"] = contract_name\n",
    "    return series\n",
    "\n",
    "\n",
    "def load_contracts(config, symbol, start_date=None, end_date=None):\n",
    "    directory = config[\"data_freq\"]\n",
    "    contract_names = [\n",
    "        x.basename().namebase\n",
    "        for x in (config[\"DATA_DIR\"] / directory).files(\"*{}*\".format(symbol))\n",
    "    ]\n",
    "    loaded = [load_contract(config, x, directory) for x in contract_names]\n",
    "    loaded = list(sorted(loaded, key=lambda x: x.index[-1]))\n",
    "    first = loaded[0]\n",
    "    # cut out from later contracts what former contracts already have\n",
    "    zipped = zip(loaded, loaded[1:])\n",
    "    cut_contracts = [\n",
    "        latter.truncate(before=former.index[-1] + pd.Timedelta(minutes=1))\n",
    "        for former, latter in zipped\n",
    "    ]\n",
    "\n",
    "    concatted = pd.concat([first] + cut_contracts)\n",
    "    return concatted.truncate(before=start_date, after=end_date)\n",
    "\n",
    "\n",
    "def get_data(config, symbol, start_date, end_date):\n",
    "    # Include up to 1 year prior for feature engineering\n",
    "    # we blindly assume no code wants a longer warm-up period than that\n",
    "    return load_contracts(\n",
    "        config,\n",
    "        symbol,\n",
    "        start_date - relativedelta(years=1) if start_date else None,\n",
    "        end_date,\n",
    "    )\n",
    "\n",
    "\n",
    "def process_bars(bars, size, type_, resolution=\"MIN\"):\n",
    "    # Renaming our bar columns & format for mlfinlab for processing and then back into our original format\n",
    "    # OHL from 1-min bars are ignored\n",
    "    fun = {\n",
    "        \"time\": get_time_bars,\n",
    "        \"tick\": get_tick_bars,\n",
    "        \"volume\": get_volume_bars,\n",
    "        \"dollar\": get_dollar_bars,\n",
    "    }[type_]\n",
    "\n",
    "    bars = bars[['Close', 'Volume']].reset_index()\n",
    "    bars.columns = ['date_time', 'close', 'volume']\n",
    "    if type_ == \"time\":\n",
    "        s_bars = fun(bars, resolution=resolution, num_units=size)\n",
    "    else:\n",
    "        s_bars = fun(bars, threshold=size)\n",
    "    bars = s_bars[['date_time', 'open', 'high', 'low', 'close', 'volume', 'cum_dollar_value', 'cum_ticks', 'cum_buy_volume']]\n",
    "    bars.columns = ['Time', 'Open', 'High', 'Low', 'Close', 'Volume', 'Dollar Volume', 'Num Ticks', 'Buy Volume']\n",
    "    if type_ == \"time\":\n",
    "        bars = bars.set_index(s_bars['date_time'].apply(datetime.fromtimestamp).values)\n",
    "    else:\n",
    "        bars = bars.set_index('Time', drop=False)\n",
    "    return bars\n",
    "\n",
    "\n",
    "def load_and_sample_bars(config, symbol, start_date, end_date, type_, size=None):\n",
    "    bars = get_data(config, symbol, start_date, end_date)\n",
    "    bars[\"Dollar Volume\"] = bars[\"Volume\"] * bars[\"Close\"]\n",
    "\n",
    "    if size is None:\n",
    "        size = determine_bar_size(bars, type_)\n",
    "\n",
    "    return process_bars(bars, size, type_), size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def determine_bar_size(bars, bar_type):\n",
    "    # Return bar size to have approx. 25 bars per day for the year 2019\n",
    "    col = {\"dollar\": \"Dollar Volume\", \"volume\": \"Volume\"}[bar_type]\n",
    "    bar_size = bars[bars.index.year == 2019][col].sum() / 252 / 25\n",
    "    return bar_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export \n",
    "\n",
    "def safe_feat_name(feat_c, safe_for_fs=True):\n",
    "    \"\"\"Turn our {feature:dict} into a pythonic function_invocation(like=string) for display & file-path purposes\"\"\"\n",
    "    feat_c = feat_c.copy()\n",
    "    for k,v in feat_c.items():\n",
    "        if isinstance(v, dict):\n",
    "            feat_c[k] = safe_feat_name(v, safe_for_fs)\n",
    "\n",
    "    if safe_for_fs:\n",
    "        name = feat_c.pop(\"name\")\n",
    "        dumped = json.dumps(\n",
    "            feat_c,\n",
    "            sort_keys=True,\n",
    "            separators=(',', '=')\n",
    "        ).replace('\"', '').replace('{', '(').replace('}', ')')\n",
    "        return f\"{name}{dumped}\"\n",
    "    else:\n",
    "        return json.dumps(\n",
    "            feat_c,\n",
    "            sort_keys=True,\n",
    "        )\n",
    "\n",
    "def load_hdf(path):\n",
    "    try:\n",
    "        if path.exists() and path.size:\n",
    "            return pd.read_hdf(path, 'table')\n",
    "    except:\n",
    "        logging.exception(\"could not load_hdf\")\n",
    "        # Does this fix our weird error?\n",
    "        with open(path) as f:\n",
    "            pass\n",
    "\n",
    "\n",
    "def save_hdf(obj, path):\n",
    "    obj.to_hdf(path, 'table')\n",
    "    return path\n",
    "\n",
    "\n",
    "def bars_path(symbol, c):\n",
    "    return c[\"DATA_DIR\"] / c[\"bar_type\"] / f\"{symbol}_bars.h5\"\n",
    "\n",
    "\n",
    "def events_b_path(symbol, c):\n",
    "    return c[\"DATA_DIR\"] / c[\"bar_type\"] / f\"{symbol}_events_{c['vol_estimate']}_{c['binarize']}_{c['binarize_params']}_{c['downsampling']}.h5\"\n",
    "\n",
    "\n",
    "def feats_path(symbol, c):\n",
    "    feat_names = '-'.join(sorted(set(x.split('_')[0] for x in c['features'])))\n",
    "    return c[\"DATA_DIR\"] / c[\"bar_type\"] / f\"{symbol}_feats_{feat_names}.h5\"\n",
    "\n",
    "\n",
    "def feat_path(c, feat_c):\n",
    "    # Make a compact, unique path for this feature config\n",
    "    basename = safe_feat_name(feat_c)\n",
    "    return c[\"DATA_DIR\"] / \"features\" / c[\"bar_type\"] / f\"{basename}.h5\"\n",
    "\n",
    "def find_params(d, s=None):\n",
    "    s = s or set()\n",
    "    for k, v in d.items():\n",
    "        if isinstance(v, dict):\n",
    "            return find_params(v, s)\n",
    "        else:\n",
    "            s.add(str(v))\n",
    "        s.add(str(k))\n",
    "    return s\n",
    "\n",
    "def imp_path(symbol, c):\n",
    "    feat_names = set.union(*[find_params(feat) for feat in c['features']])\n",
    "    feat_names = ''.join([x[:2] for x in sorted(feat_names - {'name', 'window', 'stdev', 'lag'})])\n",
    "    return c[\"DATA_DIR\"] / c[\"bar_type\"] / f\"{symbol}_fimp_{c['binarize']}_{c['binarize_params']}_{c['alpha']}_{c['alpha_params']}_{feat_names}_{c['feat_imp_method']}.h5\"\n",
    "\n",
    "\n",
    "def payload_path(symbols, c):\n",
    "    symbols_s = '-'.join(c['symbol_groups'] or c['symbols'])\n",
    "    return c[\"DATA_DIR\"] / \"payloads\" / f\"payload_{symbols_s}_{c['bar_type']}_{c['binarize']}_{c['binarize_params']}_{c['alpha']}_{c['alpha_params']}_{c['classifier']}.json\"\n",
    "\n",
    "###\n",
    "\n",
    "\n",
    "def load_bars(symbol, config):\n",
    "    if config[\"load_from_disk\"]:\n",
    "        path = bars_path(symbol, config)\n",
    "        return load_hdf(path)\n",
    "\n",
    "\n",
    "def save_bars(symbol, config, bars):\n",
    "    if config[\"save_to_disk\"]:\n",
    "        path = bars_path(symbol, config)\n",
    "        return save_hdf(bars, path)\n",
    "\n",
    "\n",
    "def load_events_b(symbol, config):\n",
    "    if config[\"load_from_disk\"]:\n",
    "        path = events_b_path(symbol, config)\n",
    "        return load_hdf(path)\n",
    "\n",
    "\n",
    "def save_events_b(symbol, config, events_b):\n",
    "    if config[\"save_to_disk\"]:\n",
    "        path = events_b_path(symbol, config)\n",
    "        return save_hdf(events_b, path)\n",
    "\n",
    "\n",
    "def load_feat(config, feat_config):\n",
    "    if config[\"load_from_disk\"]:\n",
    "        path = feat_path(config, feat_config)\n",
    "        return load_hdf(path)\n",
    "\n",
    "\n",
    "def save_feat(config, feat_config, feat):\n",
    "    if config[\"save_to_disk\"]:\n",
    "        path = feat_path(config, feat_config)\n",
    "        return save_hdf(feat, path)\n",
    "\n",
    "\n",
    "def load_imp(symbol, config):\n",
    "    if config[\"load_from_disk\"]:\n",
    "        path = imp_path(symbol, config)\n",
    "        return load_hdf(path)\n",
    "\n",
    "\n",
    "def save_imp(symbol, config, imp):\n",
    "    if config[\"save_to_disk\"]:\n",
    "        path = imp_path(symbol, config)\n",
    "        return save_hdf(imp, path)\n",
    "\n",
    "\n",
    "def load_payload(symbols, config):\n",
    "    if config[\"load_from_disk\"]:\n",
    "        path = payload_path(symbols, config)\n",
    "        try:\n",
    "            if path.exists() and path.size:\n",
    "                with open(path) as f:\n",
    "                    return json.load(f)\n",
    "        except:\n",
    "            logging.error(f\"corrupted payload: {path}\")\n",
    "\n",
    "\n",
    "def save_payload(symbols, config, payload):\n",
    "    if config[\"save_to_disk\"]:\n",
    "        path = payload_path(symbols, config)\n",
    "        with open(path, 'w') as f:\n",
    "            json.dump(payload, f, cls=NumpyEncoder)\n",
    "        return path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'feat' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-cef75ea870eb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m      'symbol': {'name': 'ffd', 'd': 0.5}}}\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0msafe_feat_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'feat' is not defined"
     ]
    }
   ],
   "source": [
    "feat1 = {'name':'a', 'b':2, 'c':3, 'd':{'name': 'dede', 'bb':33}}\n",
    "feat2=   {'name': 'auto',\n",
    "    'window': 250,\n",
    "    'lag': 10,\n",
    "    'symbol': {'name': 'stdev',\n",
    "     'window': 250,\n",
    "     'symbol': {'name': 'ffd', 'd': 0.5}}}\n",
    "\n",
    "safe_feat_name(feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_path('arst', {'features': [feat1, feat2], 'bar_type': 'derp', 'binarize': 'yes', 'binarize_params': 'yes', 'alpha': 'no', 'alpha_params': 'no', 'feat_imp_method': 'ya'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Path('C:\\\\Users\\\\doda/Dropbox/algotrading/data')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55083, 1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_hdf(DATA_DIR / 'features' / 'dollar' / 'corr(other=QCL#C,symbol=@TY#C,window=500).h5','table').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
