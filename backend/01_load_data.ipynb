{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\doda\\anaconda3\\envs\\metal\\lib\\site-packages\\sklearn\\utils\\deprecation.py:144: FutureWarning: The sklearn.ensemble.bagging module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.ensemble. Anything that cannot be imported from sklearn.ensemble is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "c:\\users\\doda\\anaconda3\\envs\\metal\\lib\\site-packages\\sklearn\\utils\\deprecation.py:144: FutureWarning: The sklearn.ensemble.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.ensemble. Anything that cannot be imported from sklearn.ensemble is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# export\n",
    "\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import json\n",
    "import logging\n",
    "from path import Path\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from mlfinlab.data_structures import get_dollar_bars, get_tick_bars, get_volume_bars\n",
    "\n",
    "from mlbt.utils import NumpyEncoder\n",
    "\n",
    "# You'll likely have to change these if you're intending to run the code yourself\n",
    "# TODO: Factor out into settings.py file\n",
    "DATA_DIR = Path(\"~/Dropbox/algotrading/data\").expanduser()\n",
    "F_PAYLOAD_DIR = Path(\"~/pr/fincl/frontend/public/payloads\").expanduser()\n",
    "DAILY_DATA_DIR = DATA_DIR / \"daily\"\n",
    "\n",
    "SYMBOLS_CSV = pd.read_csv(DATA_DIR / \"symbols.csv\", index_col=\"iqsymbol\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "def get_symbols(symbol_groups):\n",
    "    lists = {\"us_index\": [\"@NQ#C\", \"@ES#C\", \"@YM#C\"]}\n",
    "    if len(symbol_groups) == 1 and symbol_groups[0] in lists:\n",
    "        return lists[symbol_groups[0]]\n",
    "\n",
    "    sectors = {\n",
    "        \"agriculture\": \"Agriculture\",\n",
    "        \"currency\": \"Currency\",\n",
    "        \"energy\": \"Energy\",\n",
    "        \"equity_index\": \"Equity Index\",\n",
    "        \"interest_rate\": \"Interest Rate\",\n",
    "        \"metals\": \"Metals\",\n",
    "    }\n",
    "    symbol_groups = [sectors[x] for x in symbol_groups]\n",
    "    picked = SYMBOLS_CSV[SYMBOLS_CSV[\"Sector\"].isin(symbol_groups)]\n",
    "\n",
    "    ignore = [\"@LH#C\"]\n",
    "    return [x for x in picked.index.values if x not in ignore]\n",
    "\n",
    "\n",
    "def load_contract(contract_name, directory):\n",
    "    series = pd.read_csv(\n",
    "        DATA_DIR / directory / \"{}.csv\".format(contract_name), index_col=0\n",
    "    )\n",
    "    series = series[::-1]\n",
    "    if directory == \"minutely\":\n",
    "        series[\"Time\"] = series[\"date\"] + \" \" + series[\"time\"]\n",
    "        series = series.set_index(\n",
    "            pd.to_datetime(series[\"Time\"], format=\"%Y-%m-%d 0 days %H:%M:00.000000000\")\n",
    "        )\n",
    "    else:\n",
    "        series[\"Time\"] = series[\"date\"]\n",
    "        series = series.set_index(pd.to_datetime(series[\"Time\"], format=\"%Y-%m-%d\"))\n",
    "\n",
    "    series = series[[\"open_p\", \"close_p\", \"prd_vlm\", \"Time\"]]\n",
    "    series = series.rename(\n",
    "        columns={\"close_p\": \"Close\", \"open_p\": \"Open\", \"prd_vlm\": \"Volume\"}\n",
    "    )\n",
    "    series[\"Instrument\"] = contract_name\n",
    "    return series\n",
    "\n",
    "\n",
    "def load_contracts(symbol, directory=\"minutely\", start_date=None, end_date=None):\n",
    "    contract_names = [\n",
    "        x.basename().namebase\n",
    "        for x in (DATA_DIR / directory).files(\"*{}*\".format(symbol))\n",
    "    ]\n",
    "    loaded = [load_contract(x, directory) for x in contract_names]\n",
    "    loaded = list(sorted(loaded, key=lambda x: x.index[-1]))\n",
    "    first = loaded[0]\n",
    "    # cut out from later contracts what former contracts already have\n",
    "    zipped = zip(loaded, loaded[1:])\n",
    "    cut_contracts = [\n",
    "        latter.truncate(before=former.index[-1] + pd.Timedelta(minutes=1))\n",
    "        for former, latter in zipped\n",
    "    ]\n",
    "\n",
    "    concatted = pd.concat([first] + cut_contracts)\n",
    "    return concatted.truncate(before=start_date, after=end_date)\n",
    "\n",
    "\n",
    "def load_all_cont_contracts():\n",
    "    all_continuous_contracts = DAILY_DATA_DIR.files(\"*#C*\")\n",
    "    all_continuous_contracts = [x.basename().namebase for x in all_continuous_contracts]\n",
    "    return {name: load_contract(name, \"daily\") for name in all_continuous_contracts}\n",
    "\n",
    "\n",
    "def get_data(symbol, frequency, start_date, end_date):\n",
    "    # Include up to 1 year prior for feature engineering\n",
    "    # we blindly assume no code wants a longer warm-up period than that\n",
    "    return load_contracts(\n",
    "        symbol,\n",
    "        frequency,\n",
    "        start_date - relativedelta(years=1) if start_date else None,\n",
    "        end_date,\n",
    "    )\n",
    "\n",
    "def process_bars(bars, size, fun):\n",
    "    # Renaming our bar columns & format for mlfinlab for processing and then back into our original format\n",
    "    # OHL from 1-min bars are ignored\n",
    "    bars = bars[['Close', 'Volume']].reset_index()\n",
    "    bars.columns = ['date_time', 'close', 'volume']\n",
    "    s_bars = fun(bars, threshold=size)\n",
    "    bars = s_bars[['date_time', 'open', 'high', 'low', 'close', 'volume', 'cum_dollar_value', 'cum_ticks', 'cum_buy_volume']]\n",
    "    bars.columns = ['Time', 'Open', 'High', 'Low', 'Close', 'Volume', 'Dollar Volume', 'Num Ticks', 'Buy Volume']\n",
    "    bars = bars.set_index('Time', drop=False)\n",
    "    return bars\n",
    "\n",
    "\n",
    "def load_and_sample_bars(symbol, start_date, end_date, type_, size=None):\n",
    "    bars = get_data(symbol, \"minutely\", start_date, end_date)\n",
    "    bars[\"Dollar Volume\"] = bars[\"Volume\"] * bars[\"Close\"]\n",
    "\n",
    "    if size is None:\n",
    "        size = determine_bar_size(bars, type_)\n",
    "\n",
    "    process_bars_fun = {\n",
    "        \"time\": get_tick_bars,\n",
    "        \"volume\": get_volume_bars,\n",
    "        \"dollar\": get_dollar_bars,\n",
    "    }[type_]\n",
    "\n",
    "    return process_bars(bars, size, process_bars_fun), size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def determine_bar_size(bars, bar_type):\n",
    "    # Return bar size to have approx. 25 bars per day for the year 2019\n",
    "    col = {\"dollar\": \"Dollar Volume\", \"volume\": \"Volume\"}[bar_type]\n",
    "    bar_size = bars[bars.index.year == 2019][col].sum() / 252 / 25\n",
    "    return bar_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export \n",
    "\n",
    "def feat_safe_name(feat_c):\n",
    "    fc = feat_c.copy()\n",
    "    name = fc.pop(\"name\")\n",
    "    dumped = json.dumps(fc, sort_keys=True, separators=(',', '_')).replace('\"', '')\n",
    "    return f\"{name}_{dumped}\"\n",
    "\n",
    "def load_hdf(path):\n",
    "    if path.exists():\n",
    "        return pd.read_hdf(path, 'table')\n",
    "\n",
    "\n",
    "def save_hdf(obj, path):\n",
    "    obj.to_hdf(path, 'table')\n",
    "    return path\n",
    "\n",
    "\n",
    "def bars_path(symbol, c):\n",
    "    return DATA_DIR / c['bar_type'] / f\"{symbol}_bars.h5\"\n",
    "\n",
    "\n",
    "def events_b_path(symbol, c):\n",
    "    return DATA_DIR / c['bar_type'] / f\"{symbol}_events_{c['vol_estimate']}_{c['binarize']}_{c['binarize_params']}_{c['downsampling']}.h5\"\n",
    "\n",
    "\n",
    "def feats_path(symbol, c):\n",
    "    feat_names = '-'.join(sorted(set(x.split('_')[0] for x in c['features'])))\n",
    "    return DATA_DIR / c['bar_type'] / f\"{symbol}_feats_{feat_names}.h5\"\n",
    "\n",
    "\n",
    "def feat_path(c, feat_c):\n",
    "    # Make a compact, unique path for this feature config\n",
    "    basename = feat_safe_name(feat_c)\n",
    "    return DATA_DIR / 'features' / c['bar_type'] / f\"{basename}.h5\"\n",
    "\n",
    "\n",
    "def imp_path(symbol, c):\n",
    "    # TODO: This ignores feature paramters\n",
    "    feat_names = '-'.join(sorted(set(x['name'] for x in c['features'])))\n",
    "    return DATA_DIR / c['bar_type'] / f\"{symbol}_fimp_{c['binarize']}_{c['binarize_params']}_{c['alpha']}_{c['alpha_params']}_{feat_names}_{c['feat_imp_method']}.h5\"\n",
    "\n",
    "\n",
    "def payload_path(symbols, c):\n",
    "    symbols_s = '-'.join(c['symbol_groups'] or c['symbols'])\n",
    "    return DATA_DIR / 'payloads' / f\"payload_{symbols_s}_{c['bar_type']}_{c['binarize']}_{c['binarize_params']}_{c['alpha']}_{c['alpha_params']}_{c['classifier']}.json\"\n",
    "\n",
    "###\n",
    "\n",
    "\n",
    "def load_bars(symbol, config):\n",
    "    if config[\"load_from_disk\"]:\n",
    "        path = bars_path(symbol, config)\n",
    "        return load_hdf(path)\n",
    "\n",
    "\n",
    "def save_bars(symbol, config, bars):\n",
    "    if config[\"save_to_disk\"]:\n",
    "        path = bars_path(symbol, config)\n",
    "        return save_hdf(bars, path)\n",
    "\n",
    "\n",
    "def load_events_b(symbol, config):\n",
    "    if config[\"load_from_disk\"]:\n",
    "        path = events_b_path(symbol, config)\n",
    "        return load_hdf(path)\n",
    "\n",
    "\n",
    "def save_events_b(symbol, config, events_b):\n",
    "    if config[\"save_to_disk\"]:\n",
    "        path = events_b_path(symbol, config)\n",
    "        return save_hdf(events_b, path)\n",
    "\n",
    "\n",
    "def load_feat(config, feat_config):\n",
    "    if config[\"load_from_disk\"]:\n",
    "        path = feat_path(config, feat_config)\n",
    "        return load_hdf(path)\n",
    "\n",
    "\n",
    "def save_feat(config, feat_config, feat):\n",
    "    if config[\"save_to_disk\"]:\n",
    "        path = feat_path(config, feat_config)\n",
    "        return save_hdf(feat, path)\n",
    "\n",
    "\n",
    "def load_imp(symbol, config):\n",
    "    if config[\"load_from_disk\"]:\n",
    "        path = imp_path(symbol, config)\n",
    "        return load_hdf(path)\n",
    "\n",
    "\n",
    "def save_imp(symbol, config, imp):\n",
    "    if config[\"save_to_disk\"]:\n",
    "        path = imp_path(symbol, config)\n",
    "        return save_hdf(imp, path)\n",
    "\n",
    "\n",
    "def load_payload(symbols, config):\n",
    "    if config[\"load_from_disk\"]:\n",
    "        path = payload_path(symbols, config)\n",
    "        try:\n",
    "            if path.exists() and path.size:\n",
    "                with open(path) as f:\n",
    "                    return json.load(f)\n",
    "        except:\n",
    "            logging.error(f\"corrupted payload: {path}\")\n",
    "\n",
    "\n",
    "def save_payload(symbols, config, payload):\n",
    "    if config[\"save_to_disk\"]:\n",
    "        path = payload_path(symbols, config)\n",
    "        with open(path, 'w') as f:\n",
    "            json.dump(payload, f, cls=NumpyEncoder)\n",
    "        return path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
